Sqoop Commands:
----------------

1.
List all options(tools)
------------------------
sqoop help

sqoop help export

or

sqoop import --help


2.
To see the version
-------------------
sqoop version

3.
List Tables:
------------
sqoop list-tables --connect jdbc:mysql://cloudazure.eastus2.cloudapp.azure.com:3306/mysql  --username root --password hcluser


3.

MySQL Table: This is already available no need to do it
------------------------------------------------------------


use mysql;

create table AIRPORTS (code varchar(10), name varchar(100), city varchar(100),state varchar(10),country varchar(50),latitude double,longitude double);

INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('00M','Thigpen','Bay Springs','MS','USA',31.95376472,-89.23450472);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('00R','Livingston Municipal','Livingston','TX','USA',30.68586111,-95.01792778);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('Y74','Hankins','Parshall','ND','USA',47.93640083,-102.1421142);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('Y83','Sandusky City','Sandusky','MI','USA',43.45418694,-82.84938028);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('Y93','Atlanta Municipal','Atlanta','MI','USA',45.00000833,-84.13333667);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('YAK','Yakutat','Yakutat','AK','USA',59.50336056,-139.6602261);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('YAP','Yap International','NA','NA','Federated States of Micronesia',9.5167,138.1);
INSERT INTO AIRPORTS (code,name,city,state,country,latitude,longitude) VALUES ('YIP','Willow Run','Detroit','MI','USA',42.2379275,-83.53040889);

Select * from AIRPORTS;



--------------------------------------------
Importing Data
---------------------------------------------

4.

Loading into Hive Table
----------------------------
hive> drop table airports;

sqoop create-hive-table --connect jdbc:mysql://localhost:3306/mysql --table AIRPORTS --username root --password hcluser


Verify that Hive table “AIRPORTS” was actually created from inside hive
hive >
show tables;

Verify the structure is correct
describe airports;

! hadoop fs -ls /user/hive/warehouse;

quit;


/user/user1/AIRPORTS folder if exists should be deleted before executing import command (Temporary HDFS location)
hadoop fs -ls
hadoop fs -rmr /user/user1/AIRPORTS

sqoop import -m 1 --connect jdbc:mysql://localhost:3306/mysql --table AIRPORTS --username root --password hcluser --hive-import 
Verify the data was loaded in the Hive table using the following  command at Hive shell:


hive>
select count(*) from airports;
This will show the no. of records loaded into the flights table using Sqoop.


! hadoop fs -ls /user/hive/warehouse/airports;

quit;



5.
Loading into HDFS
-----------------------

hadoop fs -ls
AIRPORTS folder will not be available, If exists, delete


sqoop import -m 1 --connect jdbc:mysql://localhost:3306/mysql --table AIRPORTS --username root --password hcluser

The output can be found on HDFS under a folder name which is the name of the table i.e. in this case AIRPORTS
hadoop fs -ls


The output will be the conventional “part-m-nnnnn” file as generated by standard Mappers in MapReduce programs
which has data in comma delimited format
hadoop fs -ls AIRPORTS


6.Query on DB:
---------------
sqoop eval --connect jdbc:mysql://localhost:3306/mysql --username root --password hcluser --query "select * from AIRPORTS limit 5"


