spark

/* Databricks has sample event data as files in/databricks-datasets/structured-streaming/events/
There are about 50 JSON files in the directory. 
Each line in the files contain a JSON record with two fields - time and action
*/


%fs ls /databricks-datasets/structured-streaming/events/

%fs head /databricks-datasets/structured-streaming/events/file-0.json


/* Since the sample data is just a static set of files, you can emulate a stream from them by reading one file at a time, in the chronological order in which they were created.
*/

import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val inputPath = "/databricks-datasets/structured-streaming/events/"

// Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)
val jsonSchema = new StructType().add("time", TimestampType).add("action", StringType)

val streamingInputDF = 
  spark
    .readStream                       // `readStream` instead of `read` for creating streaming DataFrame
    .schema(jsonSchema)               // Set the schema of the JSON data
    .option("maxFilesPerTrigger", 1)  // Treat a sequence of files as a stream by picking one file at a time
    .json(inputPath)


val streamingCountsDF = 
  streamingInputDF
    .groupBy($"action", window($"time", "1 hour"))
    .count()

// Is this DF actually a streaming DF?
streamingCountsDF.isStreaming


/* You start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the complete set of 1 hour counts to be in an in-memory table.
query is a handle to the streaming query named counts that is running in the background. This query continuously picks up files and updates the windowed counts.
*/

val query =
  streamingCountsDF
    .writeStream
    .format("memory")        // memory = store in-memory table (for testing only in Spark 2.0)
    .queryName("counts")     // counts = name of the in-memory table
    .outputMode("complete")  // complete = all the counts should be in the table
    .start()

/* This query is continuously picking up files and updating the windowed counts.Let's wait a bit for a few files to be processed and then query the in-memory counts table. */

Thread.sleep(5000) // wait a bit for computation to start



%sql select action, date_format(window.end, "MMM-dd HH:mm") as time, count from counts order by time, action
/* We see the timeline of windowed counts (similar to the static one ealrier) building up. If we keep running this interactive query repeatedly, we will see the latest updated counts which the streaming query is updating in the background.
*/


Thread.sleep(5000) // wait a bit for computation to start


%sql select action, date_format(window.end, "MMM-dd HH:mm") as time, count from counts order by time, action
/* Run Again */


%sql select action, sum(count) as total_count from counts group by action order by action
/* Also, let's see the total number of "opens" and "closes" */


Thread.sleep(5000) // wait a bit for computation to start


%sql select action, sum(count) as total_count from counts group by action order by action
/* Run Again */


query.explain() 


query.name        // get the name of the auto-generated or user-specified name

query.stop()      // stop the query

query.recentProgress  // an array of the most recent progress updates for this query

